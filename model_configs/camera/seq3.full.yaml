checkpoint_interval: 5000
eval_interval: 500
log_interval: 50
batch_size: 128
epochs: 5
num_workers: 0
pack: True
plot_norms: True

lr: 0.0003
weight_decay: 0.

prior: lm_prior

data:
  train_path: gigaword/train.article.txt
  val_path:   gigaword/dev/valid.article.filter.4K.txt
  ref_path:   gigaword/dev/valid.title.filter.4K.txt
  seq_len: 50
  oovs: 10
  swaps: 0.0
vocab:
  embeddings: glove.6B.100d.txt
  embeddings_dim: 100
  size: 15000
model:
  clip: 1
  debug: True
  pack: True

  ################################################
  # LOSSES
  ################################################

  #------------------------------------
  # Reconstruction
  #------------------------------------
  loss_weight_reconstruction: 1

  #------------------------------------
  # Prior
  #------------------------------------
  grounding_loss: True
  grounding_loss_tau: 1
  loss_weight_grounding: 0.1 # [0.01 , 0.2]

  #------------------------------------
  # Topic
  #------------------------------------
  centroid_loss: True
  loss_weight_centroid: 1 # [1, 0.1]
  centroid_idf: True
  centroid_distance: cosine # cosine, euclidean

  #------------------------------------
  # Length
  #------------------------------------
  length_loss: True
  loss_weight_length: 0.01


  ################################################
  # SUMMARY LENGTHS
  ################################################
  min_ratio: 0.4
  max_ratio: 0.6
  min_length: 6
  max_length: 20
  test_min_ratio: 0.5
  test_max_ratio: 0.51
  test_min_length: 6
  test_max_length: 20

  ################################################
  # PARAMETER SHARING
  ################################################
  tie_decoder_outputs: True
  tie_embedding_outputs: True
  tie_embedding: True
  tie_decoders: False
  tie_encoders: True

  ################################################
  # INIT DECODER
  ################################################
  length_control: True
  bridge_hidden: True
  bridge_non_linearity: tanh

  emb_size: 100
  embed_noise: 0.
  embed_dropout: 0.0
  embed_trainable: True
  embed_masked: False
  layer_norm: True
  enc_token_dropout: 0.0
  dec_token_dropout: 0.5
  enc_rnn_size: 300
  dec_rnn_size: 300
  rnn_layers: 2
  rnn_dropout: 0.0
  rnn_bidirectional: True
  attention: True
  attention_fn: general
  attention_coverage: False
  input_feeding: True
  input_feeding_learnt: True
  out_non_linearity: tanh

  sampling: 0.0
  top: False
  hard: True
  gumbel: True
  tau: 0.5
  learn_tau: False
  tau_0: 0.5